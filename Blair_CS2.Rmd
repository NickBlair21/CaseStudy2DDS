---
title: "CS2"
author: "Nick Blair"
date: "4/18/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---
Package installation and library initialization
```{r}

###Please ensure that all the below packages are installed and initialized by uncommenting and running the following code.

# #install package and load library
# install.packages("stringr")
# install.packages("dplyr")
# install.packages("ggplot2")
# install.packages("tidyverse")
# install.packages("tidyr")
# install.packages("plotly")
# install.packages("class")
#install.packages("caret", dependencies = TRUE)
# install.packages("e1071")
# install.packages("GGally")
# install.packages("Hmisc")
#install.packages("corrplot")
#install.packages("randomForest")
# install.packages("ROCR")

# 
# library("stringr")
# library("dplyr")
# library("ggplot2")
# library("tidyverse")
# library("tidyr")
# library("plotly")
# library(class)
# library(caret)
# library(e1071)
# library(GGally)
# library(Hmisc)
# library(corrplot)
# library(randomForest)
# library(ROCR)

##Use the first option to select the location of relevant data on your own hard drive.
#fullData = read.csv(file.choose(), header=TRUE) #Select the location of the base data, provided as "CaseStudy2-data.csv"

fullData <- read.csv("C:/Users/37828002/OD/@@Data Science/DS 6306 Doing Data Science/Case Study 2/CaseStudy2DDS/ProvidedData/CaseStudy2-data.csv", header=TRUE)

```

Function definitions
```{r}
###Function definitions


# ++++++++++++++++++++++++++++
# flattenCorrMatrix
# Adopted from: http://www.sthda.com/english/wiki/correlation-matrix-formatting-and-visualization
# ++++++++++++++++++++++++++++
# cormat : matrix of the correlation coefficients
# pmat : matrix of the correlation p-values
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}


# Normalization
# Adopted from: https://www.edureka.co/blog/knn-algorithm-in-r/#Practical%20Implementation%20Of%20KNN%20Algorithm%20In%20R
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x))) }
```

Section 1: Exploratory Data Anaylsis
Questions:
- What are the Top 3 factors that contribute to turnover?
- Are there any job/role specific trends that exist?
- Are there any other interesting trends or observations?
```{r}

### Exploratory Data Analysis
### Our main variable of interest is Attrition which tells us whether or not an employee suffered attrition (did not return to the company).
### Specifically, we want to look for correlations between attrition and the other variables in the dataset.
### Further, we have interest in other overall interesting trends or observations, as well as any job/role specific trends that exist.



##First, we observe the values that are ordinal in nature. These variables represent categories with a clear internal progression.
##For example, JobInvolvement (from 1: Low to 4: Very High) or BusinessTravel after being converted (from 1: Non-Travel to 3: Travel Frequently),

#Create a smaller data set that only includes numeric data
#DailyRate, DistanceFromHome, Education, EnvironmentalSatisfaction, HourlyRate, JobInvolvement, JobLevel, JobSatisfaction, MonthlyRate, MonthlyIncome, NumCompaniesWorked, 
#PercentSalaryHike, PerformanceRating, RelationshipSatisfaction, StockOptionLevel, TotalWorkingYears, TrainingTimesLastYear, WorkLifeBalance, YearsAtCompany, YearsInCurrentRole, 
#YearsSinceLastPromotion, YearsWithCurrManager
numericData = fullData %>% select(ID, Attrition, Age, DailyRate, DistanceFromHome, Education,EnvironmentSatisfaction, HourlyRate, JobInvolvement, JobLevel, JobSatisfaction, MonthlyRate, MonthlyIncome, NumCompaniesWorked,PercentSalaryHike, PerformanceRating, RelationshipSatisfaction, StockOptionLevel, TotalWorkingYears, TrainingTimesLastYear, WorkLifeBalance, YearsAtCompany, YearsInCurrentRole,YearsSinceLastPromotion, YearsWithCurrManager)

#Convert Attrition factor levels from string to numeric (1 = "No", 2 = "Yes")
numericData$Attrition <- as.numeric(numericData$Attrition)

#Convert BusinessTravel from Factors ("Non-Travel", "Travel_Rarely", "Travel_Frequently") to numeric values in correct order (1 = Non-Travel, 2 = Travel_Rarely, 3 = Travel_Frequently)
numericData$NumBusinnessTravel = as.numeric(factor(fullData$BusinessTravel, levels = c("Non-Travel", "Travel_Rarely", "Travel_Frequently")))

#Create a dataset without ID numbers. ID numbers make the dataset more human readable, no ID numbers simplify later analysis
noIDnumericData = subset(numericData, select = -c(ID))


#Create a correlation matrix between all numeric variables and round them to 3 decimal places
correlationMatrix <- cor(noIDnumericData)
round(correlationMatrix, 3)




#Excludes Attribution correlation with itself and displays a summary of correlations to Attribution
#This allows us to quickly begin finding the range of highest and lowest correlation co-efficients
correlationMatrix[2,2] = NA
summary(correlationMatrix[,2])

#Produces a correlation graph between all numeric variables
#This allows us to see how all variables correlate to each other and specifically identify variables that strongly correlate to Attrition
ggcorr(noIDnumericData, label = TRUE, label_round = 2)

#Another form of correlation graph
#Requires package: corrplot
corrplot(correlationMatrix, type = "upper", order = "hclust")




#Use to produce various graphs comparing specific variables
#This allows us to take a more detailed look at how different variables relate to Attrition
ggpairs(noIDnumericData, columns = c("Attrition", "JobInvolvement", "MonthlyIncome"), upper = list(continuous = wrap("cor", size = 10)), lower = list(continuous = "smooth"))
##The default provided setup demonstrates that MonthlyIncome (and variables that share a correlation to MonthlyIncome) have a significant impact on Attrition.
##The higher the monthly income, the lower attrition.


#Create a Correlation Matrix with Significance Levels (p-value)
#Requires Hmisc package
significantCorrelationMatrix <- rcorr(as.matrix(noIDnumericData))

significantCorrelationMatrix$r
summary(significantCorrelationMatrix$P)


pvaluesSignificantCorrelationMatrix = significantCorrelationMatrix$P

pvaluesSignificantCorrelationMatrix[,"Attrition"]
pvaluesSignificantCorrelationMatrix["Attrition",]


flattenedCorrelationMatrix <- flattenCorrMatrix(significantCorrelationMatrix$r, significantCorrelationMatrix$P)
attritionFCM = flattenedCorrelationMatrix %>% filter(row == 'Attrition')
# orderedAttritionFCM <- attritionFCM[order(p),] ##this one doesn't work

#Creates a correlogram combined with a significance test
corrplot(significantCorrelationMatrix$r, type="upper", order="hclust", p.mat = significantCorrelationMatrix$P, sig.level = 0.01, insig = "blank")


##Observing the results generated by the flattened correlation matrix  of numeric values sorted by Attrition, 
##we find the following values are highly significant (P < 0.01)

## Because Attrition is a factor defined as 1 = No and 2 = Yes, a negative correlation for a positively scaled value 
##(like JobInvolvement where 1 = Low & 4 = Very High) indicates that the variable has an overall positive increase on attrition 
##(or in other words, reduces attrition). A positive correlation on a positively trending variable would indicate increasing attrition. 						

# variable              Correlation       P-value
# 
# JobInvolvement	      -0.187793409	    0.000000023931720
# TotalWorkingYears	    -0.167206122	    0.000000705884500
# JobLevel	            -0.162136444    	0.000001529785000
# YearsInCurrentRole	  -0.156215707	    0.000003665017000
# MonthlyIncome	        -0.154914955	    0.000004421756000
# Age	                  -0.149383577	    0.000009657758000
# StockOptionLevel	    -0.148680303	    0.000010645370000
# YearsWithCurrManager	-0.146782245	    0.000013814210000
# YearsAtCompany	      -0.12875406	      0.000140038700000
# JobSatisfaction	      -0.107520935	    0.001492881000000
# WorkLifeBalance	      -0.089789709	    0.008050071000000

##We also find the following values are slightly significant (P < 0.05)
#
# variable                    Correlation       P-value
#
# DistanceFromHome	          0.087136293	      0.010130340000000
# NumBusinnessTravel	        0.080387068	      0.017715680000000
# EnvironmentSatisfaction	   -0.077325405	      0.022553870000000
# 


##Next, we observe variables which are categorical in nature. These variables are separated into distinct values without a sense of 
##progression (for example, Department; while internally represented as 1: Human Resources, 2: Sales, and so on, these numbers do not
##imply a sense of progression or value of any kind.

##Thus, we do not observe correlation for these variables. Instead, we can visually observe the distribution of counts across different criteria and
##percentage of Attrition experienced in each category.


##The following statements display Tile Plots of each of the relevant categorical categories: Gender, Marital Status, Department, Education Field, & Job Role
##Use the ggplotly(p) statement at the bottom after running the desired statement.
#
p = fullData %>% ggplot(aes(x = Gender)) + geom_bar(mapping = aes(fill = Attrition), width = 0.5) + theme(axis.text.x = element_text(angle=65, vjust=0.6)) + labs(title = "Attrition by category")
ggplotly(p)

p = fullData %>% ggplot(aes(x = MaritalStatus)) + geom_bar(mapping = aes(fill = Attrition), width = 0.5) + theme(axis.text.x = element_text(angle=65, vjust=0.6)) + labs(title = "Attrition by category")
ggplotly(p)

p = fullData %>% ggplot(aes(x = Department)) + geom_bar(mapping = aes(fill = Attrition), width = 0.5) + theme(axis.text.x = element_text(angle=65, vjust=0.6)) + labs(title = "Attrition by category")
ggplotly(p)

p = fullData %>% ggplot(aes(x = EducationField)) + geom_bar(mapping = aes(fill = Attrition), width = 0.5) + theme(axis.text.x = element_text(angle=65, vjust=0.6)) + labs(title = "Attrition by category")
ggplotly(p)

p = fullData %>% ggplot(aes(x = JobRole)) + geom_bar(mapping = aes(fill = Attrition), width = 0.5) + theme(axis.text.x = element_text(angle=65, vjust=0.6)) + labs(title = "Attrition by category")
ggplotly(p)


ggplotly(p)


##The following section displays the percentages of turnover by different factors: Gender, Marital Status, Department, Education Field, Job Role
## To retrieve a specific percentage, query the prop table in the order of Row,Column
## i.e.: p <- prop.table(table(fullData %>% select (Attrition, Gender)), 2)
##       p['Yes', 'Female']
##       => [1] 0.1497175

#Summary of %Attrition of employees by Gender; Yes = employee turnover
prop.table(table(fullData %>% select (Attrition, Gender)), 2)

###Attrition rates between male and female employees are very similar to one another. 

#Summary of %Attrition of employees by Marital Status; Yes = employee turnover
prop.table(table(fullData %>% select (Attrition, MaritalStatus)), 2)

###We see a relatively high level of attrition of MaritalStatus:Single employees (21.6%).

#Summary of %Attrition of employees by Department; Yes = employee turnover
prop.table(table(fullData %>% select (Attrition, Department)), 2)

#Summary of %Attrition of employees by Education field; Yes = employee turnover
prop.table(table(fullData %>% select (Attrition, EducationField)), 2)

###We see a relatively high level of attrition of EducationField: Human Resources employees (26.7%). 

#Summary of %Attrition of employees by Job Role; Yes = employee turnover
prop.table(table(fullData %>% select (Attrition, JobRole)), 2)

### We find that among all the examined categorical variables, Job Role has the highest rate of attrition for any variable with 45% of JobRole: Sales Representative 
### experiencing attrition. 




```

Section 2: Classification - building a model to predic attrition

Part 1: KNN Classifier
```{r}
#Initialize dataset with numeric data useful to the KNN classifier
knnTraining = noIDnumericData %>% select(Attrition, MonthlyIncome, MonthlyRate, DailyRate, Age, TotalWorkingYears, YearsAtCompany, HourlyRate, NumCompaniesWorked,  PercentSalaryHike, DistanceFromHome, YearsWithCurrManager, TrainingTimesLastYear, WorkLifeBalance, EnvironmentSatisfaction, YearsSinceLastPromotion, JobInvolvement,  JobSatisfaction)


#Convert Job Role into a numeric variable; Sales Representatives have significantly high levels of attrition, making them of interest in classification
knnTraining$JobRole <- fullData$JobRole
knnTraining <- knnTraining %>% mutate(JobRole = as.character(JobRole)) %>% mutate(JobRole = replace(JobRole, JobRole != 'Sales Representative', 0))
knnTraining <- knnTraining %>% mutate(JobRole = as.character(JobRole)) %>% mutate(JobRole = replace(JobRole, JobRole == 'Sales Representative', 1))


#70-30 set split
splitPercentage = 0.7
set.seed(6)

#Create training and testing sets
trainingIndicies = sample(1:nrow(subset.knnTraining)[1],round(splitPercentage * nrow(subset.knnTraining)[1]))
train = subset.knnTraining[trainingIndicies,]
test=subset.knnTraining[-trainingIndicies,]

#Create separate dataframe for Attrition
train.attrition <- knnTraining[trainingIndicies, 1]
test.attrition <- knnTraining[-trainingIndicies, 1]


#Test to optimize KNN results 
num = 50
accs.num = data.frame(accuracy = numeric(num), k = numeric(num))
for(i in 1:num)
{
  classifications = knn(train,test,train.attrition, prob = TRUE, k = i)
  table(test.attrition,classifications)
  CM = confusionMatrix(table(test.attrition,classifications))
  accs.num$accuracy[i] = CM$overall[1]
  accs.num$k[i] = i
}

#plots results of optimization; visual inspection is used to determine best K value
p<-plot(accs.num$k,accs.num$accuracy, type = "l", xlab = "k")

knn.n <- knn(train=train, test = test, cl = train.attrition, k=12)
acc.n <- 100 * sum(test.attrition == knn.n)/nrow(test.attrition)
confusionMatrix(table(knn.n, test.attrition))


###KNN Conclusion
### The KNN classifier does well in terms of predicting True Negatives (negatives meaning the employee did not suffer attrition; Sensitivity = 0.986).
### The KNN classifier, however, does very poorly in correctly predicting True Positives (Specificity = 0.047).
### This makes it not very useful for its intended purpose. As a result, a different classifier method will be tried.

```

Section 2: classification continued
Part 2 - Random Forest Classifier
```{r}
###Random Forest

#Random forest is able to more successfully handle non-numeric data so the full dataset is made available to it.
subset.rfTraining = fullData

#70-30 set split for training and testing sets
splitPercentage = 0.7
set.seed(6)

#Create training and testing sets
trainingIndicies = sample(1:nrow(subset.rfTraining)[1],round(splitPercentage * nrow(subset.rfTraining)[1]))
train = subset.rfTraining[trainingIndicies,]
test=subset.rfTraining[-trainingIndicies,]


#Create the Random Forest model
RFmodel<-randomForest(Attrition ~ MonthlyIncome + MonthlyRate + DailyRate + Age + TotalWorkingYears + EmployeeNumber + YearsAtCompany + HourlyRate + NumCompaniesWorked
             + PercentSalaryHike + DistanceFromHome + YearsWithCurrManager + TrainingTimesLastYear
             + WorkLifeBalance + EnvironmentSatisfaction + YearsSinceLastPromotion + OverTime
             + JobInvolvement + JobSatisfaction, data = train, method = 'rf', trControl = trainControl(method = 'cv', number=5, mtry = 7, savePredictions = "final"))

predictRF = predict(RFmodel, newdata=test)
table(test$Attrition, predictRF)
##Results:
##     predictRF
  #      No Yes
  # No  215   3
  # Yes  35   8


###Conclusion
### The random forest classifier performs much better, with high approximate Sensitivity (0.878) and Specificity (0.727) values.
### As a result, the RF classifier will be used to make a prediction on the provided dataset.

#Plot ROC curve to visual represent RF success
RF_ROC=predict(RFmodel,test,type="prob")
pred_RF=prediction(RF_ROC[,2],test$Attrition)
perf_RF=performance(pred_RF,"tpr","fpr")

auc_RF <- performance(pred_RF,"auc")
auc_RF <- round(as.numeric(auc_RF@y.values),3)

print(paste('AUC of Random Forest:',auc_RF))

plot(perf_RF, col='green3')


###Testing provided dataset
CaseStudy2CompSetNoAttrition <- read.csv("C:/Users/37828002/OD/@@Data Science/DS 6306 Doing Data Science/Case Study 2/CaseStudy2DDS/ProvidedData/CaseStudy2CompSet No Attrition.csv", header=TRUE)

RFtest <- predict(RFmodel, CaseStudy2CompSetNoAttrition)

RFresults <- data.frame(CaseStudy2CompSetNoAttrition$ID, RFtest)

head(RFresults)

write.csv(RFresults, "Case2PredictionsBlair Attrition.csv")

```

Section 3: Prediction of MonthlyIncome
```{r}
#Create a smaller data set that only includes numeric data (copied from Section 1)
#DailyRate, DistanceFromHome, Education, EnvironmentalSatisfaction, HourlyRate, JobInvolvement, JobLevel, JobSatisfaction, MonthlyRate, MonthlyIncome, NumCompaniesWorked, 
#PercentSalaryHike, PerformanceRating, RelationshipSatisfaction, StockOptionLevel, TotalWorkingYears, TrainingTimesLastYear, WorkLifeBalance, YearsAtCompany, YearsInCurrentRole, 
#YearsSinceLastPromotion, YearsWithCurrManager
numericData = fullData %>% select(ID, Attrition, Age, DailyRate, DistanceFromHome, Education,EnvironmentSatisfaction, HourlyRate, JobInvolvement, JobLevel, JobSatisfaction, MonthlyRate, MonthlyIncome, NumCompaniesWorked,PercentSalaryHike, PerformanceRating, RelationshipSatisfaction, StockOptionLevel, TotalWorkingYears, TrainingTimesLastYear, WorkLifeBalance, YearsAtCompany, YearsInCurrentRole,YearsSinceLastPromotion, YearsWithCurrManager)

#Create testing and training sets from provided data
#70-30 set split
splitPercentage = 0.7
set.seed(6)

subset.LMTraining = numericData

trainingIndicies = sample(1:nrow(subset.LMTraining)[1],round(splitPercentage * nrow(subset.LMTraining)[1]))
numericTrainingData = subset.LMTraining[trainingIndicies,]
numericTestingData = subset.LMTraining[-trainingIndicies,]

#Initialize LM model
LMmodel <- lm(MonthlyIncome ~ TotalWorkingYears + Age + JobLevel + YearsInCurrentRole + YearsAtCompany + YearsWithCurrManager + Education + NumCompaniesWorked, numericTrainingData)

#Check summary and RMSE values
summary(LMmodel)

RSS <- c(crossprod(LMmodel$residuals))

MSE <- RSS / length(LMmodel$residuals)

RMSE <- sqrt(MSE)

RMSE

###Testing provided dataset
CaseStudy2CompSetNoSalary <- read.csv("C:/Users/37828002/OD/@@Data Science/DS 6306 Doing Data Science/Case Study 2/CaseStudy2DDS/ProvidedData/CaseStudy2CompSet No Salary.csv", header=TRUE)

LMtest <- predict(LMmodel, CaseStudy2CompSetNoSalary)

LMresults <- data.frame(CaseStudy2CompSetNoSalary$ID, LMtest)

head(LMresults)

write.csv(LMresults, "Case2PredictionsBlair Salary.csv")

###Conclusion
### Using the numeric data values from the full dataset, it was possible to predict potential MonthlyIncome values to the desired specification of <$3000. 

```
